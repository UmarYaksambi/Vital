# -*- coding: utf-8 -*-
"""SERVER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Y-QmDTShEW-NTvymkhyLOpzE1fsEBcn
"""

#!pip install flwr transformers peft torch huggingface_hub pyngrok bitsandbytes

# Import necessary libraries
import flwr as fl
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import get_peft_model, LoraConfig
from pyngrok import ngrok
import bitsandbytes as bnb
from huggingface_hub import HfApi, list_models


# Expose port 8080 via ngrok to allow external connections
# Set your authtoken
# ngrok.set_auth_token("2qtEASC7lPaEduFlaBozAynrqLt_4x5nrWB8f4hiPNGnL5r8P") # Replace YOUR_AUTHTOKEN with your actual token

# public_url = ngrok.connect(8080)
# print(f" * Ngrok tunnel \"{public_url}\" -> \"http://localhost:8080\"")

# Load the base model (BioMistral-7B in this case) using bitsandbytes
model_name = "EleutherAI/gpt-neo-1.3B"

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=False,  # This tells bitsandbytes to load the model in 8-bit precision
    device_map="auto"   # Automatically place the model on the available GPU/CPU
)

# Load PEFT (LoRA) adapter configuration and apply to the model
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)

# Federated Averaging Strategy with Model Save Callback and PEFT Integration
class HuggingFaceFedAvg(fl.server.strategy.FedAvg):
    def __init__(self, model, huggingface_save_name, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.model = model
        self.huggingface_save_name = huggingface_save_name

    def aggregate_fit(self, server_round, results, failures):
        # Perform the standard FedAvg aggregation
        aggregated_parameters = super().aggregate_fit(server_round, results, failures)

        if aggregated_parameters is not None:
            # Load aggregated parameters into the PEFT model
            state_dict = {k: torch.tensor(v) for k, v in zip(self.model.state_dict().keys(), aggregated_parameters)}
            self.model.load_state_dict(state_dict, strict=True)

            # Save the model locally
            save_name = f"{self.huggingface_save_name}-round-{server_round}"
            self.model.save_pretrained(save_name)
            tokenizer.save_pretrained(save_name)

            # Push model to Hugging Face
            self.push_to_huggingface(save_name)

            print(f"Model saved and pushed to Hugging Face: {save_name}")

        return aggregated_parameters

    def push_to_huggingface(self, model_dir):
        # Use Hugging Face API to upload the model
        api = HfApi()
        repo_name = self.huggingface_save_name  # Your Hugging Face repository name

        try:
            api.upload_folder(
                folder_path=model_dir,  # Path to the folder with the model
                repo_id=repo_name,  # Replace with your repo ID
                commit_message=f"Update model after federated learning round"
            )
            print(f"Successfully pushed the model to Hugging Face repository: {repo_name}")
        except Exception as e:
            print(f"Error pushing model to Hugging Face: {str(e)}")

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Start the server with the custom strategy
strategy = HuggingFaceFedAvg(
    model=model,
    huggingface_save_name="MedFusion-A-Federated-Finetuned-LLM",
    fraction_fit=1.0,
    min_fit_clients=5,
    min_available_clients=5,
)



fl.server.start_server(
        server_address="0.0.0.0:8000", 
        config=fl.server.ServerConfig(num_rounds=3) ,
        grpc_max_message_length = 1024*1024*1024,
        strategy = strategy
)

# fl.server.start_server(
#     server_address="0.0.0.0:8080",  # Change the port to 8081 or any unused port
#     config=fl.server.ServerConfig(num_rounds=5),
#     strategy=strategy,
# )



# Server address and port
# server_address = '127.0.0.1:8080'

# # Create the gRPC server
# server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

# # Add your service to the server
# your_proto_pb2_grpc.add_YourServiceServicer_to_server(YourService(), server)

# # Bind the server to the address and port
# server.add_insecure_port(server_address)

# # Start the server
# server.start()

# print(f"Server started on {server_address}")
# server.wait_for_termination()
